{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "import random\n",
    "\n",
    "#Useful to view the plots neatly together\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132, 7, 61, 738, 11, 12, 15, 11, 9, 4]\n",
      "[2.18635, 0.74785, 0.57745, 2.13561, 1.94608, 0.13258, 0.25909, 0.22904, 0.78271, -1.03416]\n",
      "[2.21153, 0.50433, 0.80513, 2.12461, 1.62462, 0.4173, 0.45267, -0.35877, 1.25115, -0.24959]\n",
      "[7, 12, 10, 12, 88, 16, 13, 8, 820, 14]\n",
      "[-0.55836, -0.91685, -0.15893, 0.31067, 0.5227, 0.04649, 0.45648, -0.37345, 1.43161, 0.40216]\n",
      "[-0.71167, -1.04018, 0.01069, 0.32, 0.50619, -0.07106, 0.09098, -0.3061, 1.3827, 0.26149]\n",
      "[6, 26, 9, 10, 14, 861, 46, 8, 13, 7]\n",
      "[-0.38747, 0.73966, -0.73511, -0.0271, 0.05971, 0.78746, 0.39062, 0.29293, 0.33786, -1.6438]\n",
      "[-0.32826, 0.85797, -0.46625, 0.26442, -0.15648, 0.84435, 0.40529, 0.53106, 0.43624, -1.07616]\n",
      "[7, 4, 10, 11, 8, 7, 12, 10, 923, 8]\n",
      "[-0.29131, 0.04886, -1.10589, 0.29644, -1.0283, -1.02638, -0.51356, -0.74225, 2.35969, -0.61008]\n",
      "[-0.22856, 0.75553, -0.87702, 0.36532, -1.71622, -0.20089, -0.63308, -0.5752, 2.35743, -0.89641]\n",
      "[12, 9, 897, 8, 12, 20, 10, 10, 10, 12]\n",
      "[0.29721, 0.21076, 1.33448, -0.05939, -1.70567, 0.98515, -0.77687, -0.17232, 0.58066, 0.69543]\n",
      "[-0.05129, -0.50485, 1.31695, 0.0325, -1.7479, 1.02346, -0.53527, -0.20318, 0.16623, 0.76382]\n",
      "[12, 3, 16, 902, 8, 14, 11, 9, 15, 10]\n",
      "[0.64099, -2.10472, -0.14605, 1.00887, -0.26839, -0.48795, -0.10789, -1.25587, -0.73889, 0.12727]\n",
      "[0.7284, -1.67635, 0.02388, 1.04334, -0.23531, -0.57859, -0.14167, -1.36305, -0.44718, 0.27408]\n",
      "[7, 20, 10, 9, 39, 874, 12, 11, 7, 11]\n",
      "[-0.37367, 1.93623, -0.44679, 0.14149, 0.50878, 1.96665, -2.61409, 1.48764, 0.86819, 1.08187]\n",
      "[-0.72649, 1.90221, -0.66793, 0.31763, 0.41043, 1.97767, -2.16603, 1.32091, 0.94244, 0.6855]\n",
      "[810, 12, 14, 5, 18, 8, 82, 21, 20, 10]\n",
      "[2.21364, -1.57539, -0.64328, 0.63653, 0.54243, -2.11514, 1.84334, 1.86151, 0.11649, 0.75679]\n",
      "[2.22476, -1.51881, -0.3511, -0.15088, 0.39491, -2.14823, 1.6698, 1.5267, 0.08831, 0.78513]\n",
      "[5, 14, 12, 9, 9, 13, 7, 871, 50, 10]\n",
      "[0.23081, -0.46107, -1.50783, -0.314, -0.54573, 0.38157, 0.00976, 1.44283, 0.89333, 0.58331]\n",
      "[0.54813, -0.76988, -1.24206, -0.3586, -0.32648, -0.00056, -0.35964, 1.47773, 1.13315, 0.66942]\n",
      "[17, 7, 49, 16, 10, 6, 16, 9, 857, 13]\n",
      "[-0.72815, 0.60841, 0.29674, -0.49756, -1.0962, -1.64616, 0.46797, -0.03037, 1.3361, 0.2132]\n",
      "[-0.86963, 0.53126, 0.41962, -0.83205, -0.84484, -2.13848, 0.32597, -0.36135, 1.36687, 0.55983]\n"
     ]
    }
   ],
   "source": [
    "#initialize the number of actions available to be selected\n",
    "k = range(10)\n",
    "#initialize the number of time steps in a run\n",
    "t = range(1000)\n",
    "#initialize the number of runs as the size of the bandit problem testbed\n",
    "runs = range(10)\n",
    "#initialize the level of epsilon-greedy action selection\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "#bandit testbed simulation loop\n",
    "for run in runs:\n",
    "    \n",
    "#initialize the initial estimated values as expected rewards\n",
    "    q_est = [0 for i in k]\n",
    "#initialize action selection counter for reward sample mean\n",
    "    n_act = [0 for i in k]\n",
    "    \n",
    "#initialize the actual values\n",
    "    q_val = [np.random.normal() for i in k]\n",
    "    \n",
    "#initialize sets of random variables to be used in action selection\n",
    "    rn1 = [random.random() for r in t]\n",
    "    rn2 = [random.random() for r in t]\n",
    "    rn3 = [random.random() for r in t]\n",
    "    \n",
    "#loop through the timesteps for action selections\n",
    "    for x in t:\n",
    "\n",
    "#choose an action from the action set\n",
    "#with probability 1-epsilon, choose the greedy action\n",
    "        if rn1[x] < (1-epsilon):\n",
    "            max_ind = [ind for ind,q in enumerate(q_est) if q == max(q_est)]\n",
    "            max_i_r = int(math.floor(rn2[x]*len(max_ind)))\n",
    "            j = max_ind[max_i_r]\n",
    "#with probability epsilon, choose a random action\n",
    "        if rn1[x] >= (1-epsilon):\n",
    "            j = int(math.floor(rn3[x]*len(q_est)))\n",
    "        \n",
    "#the actual reward of the chosen action as a stochastic variable\n",
    "#with mean equivalent to the actual value of the chosen action\n",
    "        r_t = np.random.normal(loc=q_val[j])\n",
    "    \n",
    "#increment the selection counter for the chosen action\n",
    "        n_act[j] += 1\n",
    "#incrementally update the action's estimated value as\n",
    "#the sample mean reward\n",
    "        q_est[j] += (1/n_act[j])*(r_t - q_est[j])\n",
    "    \n",
    "\n",
    "    print(n_act)\n",
    "    q_est = ([round(q, 5) for q in q_est])\n",
    "    print(q_est)\n",
    "    q_val = ([round(q, 5) for q in q_val])\n",
    "    print(q_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08996873321791116, -0.6270514214814528, 0.23487563568249256, -0.15576427172776225, -0.12215788805871944, -0.0019313189058612343, 0.16916607333386216, -0.1880085644692798, -0.7158600025073805, 0.30030940528925165]\n"
     ]
    }
   ],
   "source": [
    "#initialize the number of actions available to be selected\n",
    "k = range(10)\n",
    "#initialize the number of time steps in a run\n",
    "t = range(1000)\n",
    "#initialize the number of runs as the size of the bandit problem testbed\n",
    "runs = range(10)\n",
    "#initialize the level of epsilon-greedy action selection\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "#bandit testbed simulation loop\n",
    "for run in runs:\n",
    "    \n",
    "#initialize the initial estimated values as expected rewards\n",
    "    q_est = [0 for i in k]\n",
    "#initialize action selection counter for reward sample mean\n",
    "    n_act = [0 for i in k]\n",
    "    \n",
    "#initialize the actual values\n",
    "    q_val = [np.random.normal() for i in k]\n",
    "    \n",
    "#initialize sets of random variables to be used in action selection\n",
    "    rn1 = [random.random() for r in t]\n",
    "    rn2 = [random.random() for r in t]\n",
    "    rn3 = [random.random() for r in t]\n",
    "    \n",
    "#loop through the timesteps for action selections\n",
    "    for x in t:\n",
    "\n",
    "#choose an action from the action set\n",
    "#with probability 1-epsilon, choose the greedy action\n",
    "        if rn1[x] < (1-epsilon):\n",
    "            max_ind = [ind for ind,q in enumerate(q_est) if q == max(q_est)]\n",
    "            max_i_r = int(math.floor(rn2[x]*len(max_ind)))\n",
    "            j = max_ind[max_i_r]\n",
    "#with probability epsilon, choose a random action\n",
    "        if rn1[x] >= (1-epsilon):\n",
    "            j = int(math.floor(rn3[x]*len(q_est)))\n",
    "        \n",
    "#the actual reward of the chosen action as a stochastic variable\n",
    "#with mean equivalent to the actual value of the chosen action\n",
    "        r_t = np.random.normal(loc=q_val[j])\n",
    "    \n",
    "#increment the selection counter for the chosen action\n",
    "        n_act[j] += 1\n",
    "#incrementally update the action's estimated value as\n",
    "#the sample mean reward\n",
    "        q_est[j] += (1/n_act[j])*(r_t - q_est[j])\n",
    "    \n",
    "        \n",
    "    \n",
    "#    r_hist[x] = r_t\n",
    "#    for p in range(ks):\n",
    "#        reward_sum += reward_hist[x]*epsilon*(1-epsilon)^(m-i??\n",
    "#    q_act[j] = ((1-epsilon)^n_act[j])*q_init[j] + (reward_hist[i][j])*epsilon*(1-epsilon)^(n_act[j]-i)\n",
    "    \n",
    "\n",
    "    print(n_act)\n",
    "    q_est = ([round(q, 5) for q in q_est])\n",
    "    print(q_est)\n",
    "    q_val = ([round(q, 5) for q in q_val])\n",
    "    print(q_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
